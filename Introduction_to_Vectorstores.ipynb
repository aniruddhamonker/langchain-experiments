{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Vectorstores\n",
    "\n",
    "One of the most popular ways of using Large Language Models(LLMs) have been to use an unstructured natural language query to perform a similarity search over a wide range of data sources.\n",
    "\n",
    "This is acheived by embedding all source data and storing the resulting embedding vectors in a Vectorstore. \n",
    "\n",
    "#### What is an Embedding\n",
    "\n",
    "Embeddings create a vector representation of a piece of text. \n",
    "\n",
    "<img src=\"https://cdn.openai.com/embeddings/draft-20220124e/vectors-1.svg\">\n",
    "\n",
    "This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "A Vectorstore is similar to a database that stores the embedded data and allows performing a search on that data based on the embedding vectors. \n",
    "\n",
    "\n",
    "<img src=\"https://python.langchain.com/assets/images/vector_stores-125d1675d58cfb46ce9054c9019fea72.jpg\" width=\"800\" height=\"400\">\n",
    "\n",
    "A LLM helps embed an unstructurd natural language query  and vectorstores help retrieve the embedding vectors that are 'most similar' to the embedded query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Vectorstores\n",
    "\n",
    "Two of the most common free to use vectorstores that can be installed locally are:\n",
    "* Chromadb\n",
    "* FAISS\n",
    "\n",
    "Lets look at installing and using both of these DB stores. \n",
    "\n",
    "For both demonstrations we will use the embedding model from Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain-openai chromadb\n",
    "# following import of pysqlite3 is to override the system sqlite3 version 3.31 which is unsupported by chromadb\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important entities while working with vectorstores is the use of what is called \"Langchain Document Loaders\" and \"Text/data Splitters\".\n",
    "\n",
    "Document Loaders allow ingesting various types of data sources into your program. The sources of data could be structured data like SQL , etcd databases or unstructured like web page contents, text documents, PDFs, etc.\n",
    "\n",
    "For our use case and for simplicity, we are going to use langchain's \"WebBaseLoader\", which can load a document directly from a URL.\n",
    "\n",
    "Each LLM has a restricted context window size  that limits the amount of data/information that we can provide or feed into the LLM model at a time. This is where \"Splitters\" come into picture. The source data is split into data chunks that are large enough to fit into LLM's context window size.\n",
    "\n",
    "There are different types of splitters built into \"langchain.text_splitter\" module. In this case we are going to use a simple \"CharacterTextSplitter\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beautiful soup package required for WebBaseLoader\n",
    "#! pip install bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Program flow for performing similarity search\n",
    "\n",
    "* Load the document using \"OnlinePDFLoader\"\n",
    "* split the document into chunks using \"CharacterTextsplitter\"\n",
    "* embed each chunk of the document using \"OpenAIEmbeddings\"\n",
    "* Load the embredded data into the \"Chroma\" db vectorstore\n",
    "\n",
    "Lets try to put all the above steps in to the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcast_vpn_rfc_loader= WebBaseLoader(\"https://www.rfc-editor.org/rfc/rfc6517.txt\")\n",
    "mcast_vpn_rfc_document = mcast_vpn_rfc_loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "split_documents = text_splitter.split_documents(mcast_vpn_rfc_document)\n",
    "db = Chroma.from_documents(split_documents, OpenAIEmbeddings())\n",
    "\n",
    "query =\"what is the document about?\"\n",
    "db.similarity_search(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully indexed our data chunks in a Vectorstore DB.\n",
    "\n",
    "The next step is to prompt a LLM model by providing relevant chunks of data retrieved from the vector DB similarity search to answer the user query. \n",
    "\n",
    "The way this is done is by using a \"Retriever\" or a \"Retrieval Chain\" which we will cover in the next Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a27b9a44dddb1eedec426fa966a46221b6963a983add3b1046110dc32ee2722c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
