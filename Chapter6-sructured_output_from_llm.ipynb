{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Structured Output from LLM\n",
    "\n",
    "All the very popular LLMs out there respond in a a very conversational manner with the output being an unstructured text or in a natural language. There are times where you might want to get a structured output on the user query from a LLM such as JSON or an object with certain attributes(pydantic or dataclass objects).\n",
    "\n",
    "We have seen one such example in our \"langchain_tools\" notebook where in a LLM responds to the user query and select tools from a list of avaialble tools by returning a \"ToolMessage\" object.\n",
    "\n",
    "It is important to keep in mind that not all LLMs can respond in a structured way. LLMs which have the capabilities of function or tool calling are best suited for getting structured outputs. You can find a list of LLMs that support structured output [Here](https://python.langchain.com/v0.2/docs/integrations/chat/)\n",
    "\n",
    "In this notebook , we are going to look at few of the methods that Langchain offers when it comes to requesting a structured output from a LLM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with_structured_output method\n",
    "\n",
    "Langchain have implemented this method with all the LLM models that support returning structured outputs(JSON or tool/function calling).\n",
    "\n",
    "This method takes in a JSON schema or a Pydantic class as an input and returns dictionary or a Pydantic object when invoked with \".invoke()\" method.\n",
    "\n",
    "Lets see an example of using both a JSON schema and a Pydantic class with OpenAI LLM model\n",
    "\n",
    "### Using Pydantic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain-core langchain-openai\n",
    "import os, getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "class NewRouter(BaseModel):\n",
    "    \"\"\"A class that provides a Hostname , private IP address\\\n",
    "          and a Login Banner to a new Router\"\"\"\n",
    "    \n",
    "    hostname: str = Field(..., description=\"A hostname based on the location of the device\")\n",
    "    ip_address: str = Field(..., description=\"An IPv4 address from private address space,\\\n",
    "                             easy to remember\")\n",
    "    login_banner: str = Field(..., description=\"A login banner for a router as a motivational quote\")\n",
    "\n",
    "llm_with_structure = llm.with_structured_output(NewRouter)\n",
    "\n",
    "llm_with_structure.invoke(\"provide a router hostname , and Ipv4 address and \\\n",
    "                          a login banner for a router located in san francisco\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note here that besides just defining a Pydantic class ; the name of the class , the docstring and the descriptions of the class attributes are very important as all of these metadata is passed on as context to LLM\n",
    "\n",
    "### Using JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hostname': 'sf-router',\n",
       " 'ip_address': '192.168.1.1',\n",
       " 'login_banner': 'Welcome! The future depends on what you do today.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"title\" :  \"new_router\",\n",
    "    \"description\": \"A hostname , an IPv4 address and a login banner\\\n",
    "        to provision a new router\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"hostname\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A hostname based on the location of the device\"\n",
    "        },\n",
    "        \"ip_address\":{\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"An IPv4 address from private address space,\\\n",
    "                easy to remember\"\n",
    "        },\n",
    "        \"login_banner\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A login banner for a router as a motivational quote\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"hostname\", \"ip_address\"]\n",
    "}\n",
    "\n",
    "llm_with_json_output = llm.with_structured_output(json_schema)\n",
    "\n",
    "llm_with_json_output.invoke(\"provide a router hostname , an Ipv4 address and \\\n",
    "                          a login banner for a router located in san francisco\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Structured output directly from the model\n",
    "\n",
    "Not all models support \"with_structured_output()\", since not all models have tool calling or JSON mode support. For such models langchain provides output parsers that can extract a structured response from the raw model output.\n",
    "\n",
    "We have actually covered this method in our first Notebook in this series \"Langchain Quick Introduction\".\n",
    "For the sake of completeness of this section , lets see an example of using Langchain's output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class WeatherForcast(BaseModel):\n",
    "    query:str = Field(description=\"question asked to LLM Model\")\n",
    "    response:str = Field(description=\"response to the query from LLM model\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=WeatherForcast)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{format_instructions}\\nDescribe weather forcast for today at {location}\\\n",
    "          in no more than two short sentences\",\n",
    "    input_variables=[\"location\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "llm(prompt.format(location=\"San Francisco\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
