{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with Local LLMs\n",
    "\n",
    "* Introduction\n",
    "    * Interest in the topic\n",
    "    * not Juniper product topic\n",
    "    * no slidedeck , using Jupyter Notebooks\n",
    "    * first time \n",
    "\n",
    "* what are we going to learn?\n",
    "    * two things - Local LLMs , Langchain\n",
    "    * Introductory level - Simple\n",
    "    * how you can learn -github repo\n",
    "\n",
    "* Why learn about this topic\n",
    "    * adoption of public models\n",
    "    * fits under the same umbrella but targets specific use cases\n",
    "        * challenges with public models\n",
    "            * privacy\n",
    "            * Pay per use APIs\n",
    "            * fine tuning\n",
    "    * Leverage AI to write scripts, tools, software\n",
    "    * modern way of developing software\n",
    "    * when I first started\n",
    "    * AI as a must have tool in your arsenal\n",
    "\n",
    "* Some Pre-requisites(in order to get started)\n",
    "    * Python\n",
    "        * basic data structures(dictionaries, lists)\n",
    "        * classes , objects, methods\n",
    "        * pip and packages installations and management\n",
    "    * GPTs\n",
    "        * chat GPT\n",
    "        * writing prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks\n",
    "\n",
    "* Interactive Tool designed to combine text and code and provide outputs in an organized way.\n",
    "* Very popular for analyzing data, creating models or teaching programming concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Made Easy - Github Repo\n",
    "\n",
    "* A beginner friendly guide to Langchain\n",
    "\n",
    "  https://github.com/aniruddhamonker/langchain-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "\n",
    "* GPT and Prompt Engineering\n",
    "* Inferencing vs Training\n",
    "* Parameters and Tokens\n",
    "* LLM Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is GPT?\n",
    "\n",
    "All the LLM models we interact with now a days, they are all based on what we call a GPT architecture.\n",
    "GPT (Generative Pre-trained Transformer) is an architecture based on deep learning and nueral networks, that is trained to generates human-like text responses in a conversational manner. \n",
    "Although it shows that it understands user input and text , It however relies on statistical patterns in the training and the input data.\n",
    "AI doesn’t know what it knows and it doesn’t not know why it knows what it knows\n",
    "\n",
    "So when it comes to text predictions and generating answers , the input prompt that you provide to a LLM matters. \n",
    "A more detailed and a contextually relevant prompt can generate far better answers than just a single sentence and short prompts\n",
    "Prompt engineering is really an art of devising prompts that can help user get accurate answers.\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "Parameters\n",
    "\n",
    "What does it mean when someone says , chat-gpt is a 200 Billion parameter model and is trained on 15 Trillion Tokens    \n",
    "\n",
    "Parameter are like instructions that the model uses to make accurate predictions or generate desired outputs.\n",
    "\n",
    "Generally, more parameters allow the model to learn complex patterns and relationships. However, larger models with more parameters require more computational resources and training time. \n",
    "\n",
    "These are the internal variables or weights that the AI model learns from the training data and the objective is to find the best values for the parameters that enable the model to make accurate predictions on new, unseen data\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "training of a model\n",
    " \n",
    "Pretraining is a technique where a model is first trained on a large dataset using unsupervised learning to learn the statistical properties of the input data or text. \n",
    "\n",
    "During pre-training, the model learns to predict missing words in a sentence or generate the next word in a sequence.\n",
    "\n",
    "After pre-training, the model is fine-tuned on specific tasks using what is called supervised learning, such as text generation or language translation, to adapt it to a particular application.\n",
    "\n",
    "The idea behind pretraining is to initialize the model with weights or parameters that now have better capability to make accurate predictions, which can then be fine-tuned for the specific task using supervised learning\n",
    "\n",
    "Intruct and chat are the two most popular types of fine tuned models we have today\n",
    "for e.g chat GPT is a kind of a fine-tuned version of the base GPT model.\n",
    "\n",
    "You will see as we learn about local LLMs, you will have to make a choice about what type of model \n",
    "\n",
    "there are ways to further fine tune the already fine tuned models using what is called in context learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Large Language Models(LLMs)\n",
    "\n",
    "* Large language model that can be entirely run on a local machine rather than relying on cloud based LLM model such as ChatGPT\n",
    "* Advantages of local LLMs\n",
    "* LLM Quantization\n",
    "* Running Local LLMs using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Large Language Models(LLMs)\n",
    "\n",
    "* Large language model that can be entirely run on a local machine rather than relying on cloud based LLM model such as ChatGPT\n",
    "* Advantages of local LLMs\n",
    "* LLM Quantization\n",
    "* Running Local LLMs using Ollama\n",
    "\n",
    "A Local LLM is a large language model that can be entirely run on a local machine rather than relying on cloud based LLM model such as ChatGPT.\n",
    "Running LLMs locally offers several advantages, especially in scenarios where privacy, data security, latency, and offline access are important considerations.; not to mention the costs associated with accessing public LLM APIs.\n",
    "\n",
    "You might have an impression that inferencing or training a LLM locally requires exorbitant amount of compute power (GPUs , memory, storage, etc) and that correlates to large upfront costs. Well, its partially true if you are trying to download  and host a large 75 billion parameter, trained LLM model such as llama3 on a local server.\n",
    "\n",
    "However, thanks to the technique of LLM Quantization(Q-LLM), we can download and host quantized versions of the very same 75B parameter models on our local machine or laptops. Quantization works by reducing the size and complexity of an existing LLM model weights and storing them in a smaller 8 bit formats instead of 32 bit high precision format.\n",
    "\n",
    "Quantization makes the model a whole lot lighter , faster and consumes less memory and processing power. A quantized LLM can still make predictions and generate text at the expense of bit less accuracy; but for many applications the advantages of a Q-LLM model far outweighs a slight trade-off in precision.\n",
    "\n",
    "Now that we know about Quantized LLMs and what they have to offer, the next piece to the puzzle is how do we actually run these  Q-LLM models locally. This is what \"Ollama\" is for; it is an application for running and managing large language models (LLMs) locally on personal computer or servers.\n",
    "\n",
    "Ollama supports GPU acceleration (via CUDA) and optimized memory management, to run resource-intensive LLMs efficiently. Users with compatible GPUs or other high-performance hardware can experience faster, real-time interactions with language models without needing cloud infrastructure. When I first started learning about local LLMs, you had to download a model from hugging face repository, then configure the hardware on my laptop (CPU, memory , pytorch library) etc to make it work. Ollama takes care of all this. It overly simplifies the entire workflow of downloading a model and running it locally.\n",
    "\n",
    "Ollama comes with the support for some of the most popular pre-quantized , optimized models such as Meta's Llama, Mistral , Gemma, and many others which are ready for deployment.\n",
    "\n",
    "Although Ollama can be independently run on its own, we are also going to see its integration with Langchain and how to leverage the local models from ollama within Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Installation\n",
    "\n",
    "To install ollama on a linux machine we use **install.sh** script provided by Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "\"\"\"\n",
    ">>> Installing ollama to /usr/local\n",
    ">>> Downloading Linux amd64 bundle\n",
    "######################################################################## 100.0%\n",
    ">>> Adding ollama user to render group...\n",
    ">>> Adding ollama user to video group...\n",
    ">>> Adding current user to ollama group...\n",
    ">>> Creating ollama systemd service...\n",
    ">>> Enabling and starting ollama service...\n",
    ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
    ">>> Install complete. Run \"ollama\" from the command line.\n",
    "WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama has a wide choice of LLMs that it supports and are available for download. You can choose what model you would like to download from Ollama's model library here: [Ollama Models](https://ollama.com/library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama has a wide choice of LLMs that it supports and are available for download. You can choose what model you would like to download from Ollama's model library here: [Ollama Models](https://ollama.com/library)\n",
    "\n",
    "We are going to download and use Meta's **Llama 3.2** 3B parameter model. As of writing this notebook , this model has been gaining a lot of popularity for being small (2GiB) and surprisingly efficient across multiple tasks such as following instructions, summarization, prompt rewriting, tool use , etc..\n",
    "\n",
    "With Ollama installed, lets go ahead and download llama3.2 using \"ollama pull\" command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ollama pull llama3.2\n",
    "#! ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED     \n",
      "llama3.1:latest    42182419e950    4.7 GB    47 hours ago    \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    4 days ago      \n"
     ]
    }
   ],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain - A Brief Intro!\n",
    "\n",
    "* A programming framework that simplifies the process of building powerful AI tools \n",
    "* Integrate LLMs into your existing applications or build entirely new solutions with minimal effort\n",
    "* Develop customized AI solutions for variety of use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain - A Brief Intro!\n",
    "\n",
    "Langchain is a programming framework that simplifies the process of building powerful AI tools that can perform tasks like answering questions, processing documents, automating workflows, and much more. It allows you to integrate LLMs into your existing applications or build entirely new solutions with minimal effort, giving you the flexibility to develop customized AI solutions for a variety of use cases.\n",
    "\n",
    "The \"chain\" in Langchain refers to sequence of steps or operations where different components, like language models, prompts , tools, and external resources, are connected to work together. Each step in the chain performs a specific task, such as calling an API, retrieving information, or processing data, and the output of one step can serve as the input to the next, hence the name \"chain\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building With Langchain\n",
    "\n",
    "The simplest and most common chain contains three things:\n",
    "\n",
    "* LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
    "* Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
    "* Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM/Chat Model\n",
    "\n",
    "There are two types of language models within Langchain:\n",
    "\n",
    "* LLM: underlying model takes a string as input and returns a string\n",
    "* ChatModel: underlying model takes a list of messages as input and returns a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain langchain_core langchain_ollama langchain_openai\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os, getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
    "\n",
    "ollama_llm = OllamaLLM(model=\"llama3.2\")\n",
    "chat_ollama = ChatOllama(model=\"llama3.1\")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message unlike strings is an object of type 'BaseMessage' and is composed of two attributes\n",
    "\n",
    "* content: The content of the message. Usually a string.\n",
    "* role: The entity from which the BaseMessage is coming.\n",
    "\n",
    "Langchain provides several types of messages to distinguish between the roles:\n",
    "\n",
    "* HumanMessage: A BaseMessage coming from a human/user.\n",
    "* AIMessage: A BaseMessage coming from an AI/assistant.\n",
    "* SystemMessage: A BaseMessage coming from the system.\n",
    "* FunctionMessage / ToolMessage: A BaseMessage containing the output of a function or tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = \"\"\"\n",
    "As an expert in the field of computer networking answer all the user related questions. \\\n",
    "\"\"\"\n",
    "#try to keep the answers short and concise. do not give lengthy explanations.\n",
    "\n",
    "text_input = \"how many usable host IP addresses are under subnet 192.168.10.0/24?\"\n",
    "instruct_model_input = \"write a brief email asking cusotmer for their feedback on their last support experience\"\n",
    "\n",
    "#message = [HumanMessage(content=text_input), SystemMessage(content=system_message)]\n",
    "message = [\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", text_input)\n",
    "]\n",
    "\n",
    "#ollama_instruct_response = ollama_llm.invoke(instruct_model_input)\n",
    "#ollama_chat_response = chat_ollama.invoke(message)\n",
    "openai_chat_response = chat_openai.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A /24 subnet mask indicates that the first 24 bits of the IP address are used for the network portion, leaving the remaining 8 bits for host addresses. This means there are \\(2^8 = 256\\) possible IP addresses in this subnet.\n",
      "\n",
      "However, two addresses are reserved: one for the network address (192.168.10.0) and one for the broadcast address (192.168.10.255). Therefore, the number of usable host IP addresses is \\(256 - 2 = 254\\).\n"
     ]
    }
   ],
   "source": [
    "print(openai_chat_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates are used to format user inputs to the LLMs. Instead of simply providing a text/string input like we did above, templates allow us to insert placeholders in to the text.\n",
    "\n",
    "The value for these placeholders can then be provided at runtime when the llm is called. Lets see that with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"suggest a Hostname for a networking router based on its model: {model_name} and its location at {location}\")\n",
    "\n",
    "template_response = ollama_llm.invoke(template.format(model_name =\"Juniper MX\", location=\"san francisco\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates are also available for chat models. For chat models we can put placeholders for inputs from each type of role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = system_message\n",
    "human_template = \"suggest a Hostname for a networking router based on its model: {model_name} and its location at {location}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_template_response = chat_ollama.invoke(chat_prompt.format_messages(model_name=\"Juniper MX\", location=\"san francisco\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the device model (Juniper MX) and its location (San Francisco), I would suggest the following hostname:\n",
      "\n",
      "`sfmx001`\n",
      "\n",
      "Breakdown:\n",
      "\n",
      "* `sf`: This is a 2-character abbreviation for San Francisco, which is common in IT infrastructure naming conventions.\n",
      "* `mx`: This reflects the Juniper MX model, making it clear what type of device this is and helping with troubleshooting or identification purposes.\n",
      "* `001`: This is a sequential numbering convention to avoid conflicts with other devices. You can increment this number if you have multiple routers at the same location.\n",
      "\n",
      "So, the suggested hostname is `sfmx001`, which clearly identifies the device as a Juniper MX router located in San Francisco.\n"
     ]
    }
   ],
   "source": [
    "print(chat_template_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output from the LLMs are plain text but sometimes we desire to get a more structured output from our applications rather than just plain text.\n",
    "\n",
    "This is where the output parsers are very helpful.  Although we can define our own output parser, Langchain has several pre-defined output parser classes that we can consume , instead of defining our own.\n",
    "\n",
    "One of the popular output parser is \"PydanticOutputParser\" class. Let's look at an example of using this parser class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class WeatherForcast(BaseModel):\n",
    "    query:str = Field(description=\"question asked to LLM Model\")\n",
    "    response:str = Field(description=\"response to the query from LLM model\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=WeatherForcast)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{format_instructions}\\n suggest a Hostname for a networking router \\\n",
    "        based on its model: {model_name} and its location at {location}\",\n",
    "    input_variables=[\"model_name\", \"location\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "output_parser_response = chat_ollama.invoke(prompt.format(model_name=\"Juniper SRX\", location=\"San Francisco\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the JSON schema provided, here's a suggested hostname format:\n",
      "\n",
      "`juniper-srx-san-francisco-router`\n",
      "\n",
      "Here's how it conforms to the schema:\n",
      "\n",
      "* `query`: The \"question asked to LLM Model\" is represented by the model of the router, which is \"Juniper SRX\".\n",
      "* `response`: The \"response to the query from LLL model\" is represented by the location of the router, which is \"San Francisco\".\n",
      "\n",
      "This hostname format follows the schema's requirements:\n",
      "\n",
      "* It contains both the `query` (model) and `response` (location) values.\n",
      "* Both values are strings.\n",
      "\n",
      "So, this hostname instance should be well-formatted according to the provided JSON schema.\n"
     ]
    }
   ],
   "source": [
    "print(output_parser_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chain using LCEL\n",
    "\n",
    "Langchain also provides what is known as \"Langchain Expression Language or LCEL\" which uses the \" | \" operator to combine different components together, such as prompts and instance of LLM. LCEL is a way to create arbitrary custom chains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_first_chain = prompt | chat_ollama | output_parser\n",
    "\n",
    "chain_response = our_first_chain.invoke({\n",
    "    \"model_name\": \"Juniper QFX\",\n",
    "    \"location\": \"San Francisco\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The suggested hostname could be junipera-sf-qfx-router'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_response.response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
