{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Intro!\n",
    "\n",
    "Langchain is a framework that simplifies the process of building powerful AI tools that can perform tasks like answering questions, processing documents, automating workflows, and much more. It allows you to integrate LLMs into your existing applications or build entirely new solutions with minimal effort, giving you the flexibility to develop customized AI solutions for a variety of use cases.\n",
    "\n",
    "The \"chain\" in Langchain refers to sequence of steps or operations where different components, like language models, tools, and external resources, are connected to work together. Each step in the chain performs a specific task, such as calling an API, retrieving information, or processing data, and the output of one step can serve as the input to the next, hence the name \"chain\".\n",
    "\n",
    "## Building With Langchain\n",
    "\n",
    "The simplest and most common chain contains three things:\n",
    "\n",
    "* LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
    "* Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
    "* Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM/Chat Model\n",
    "\n",
    "There are two types of language models within Langchain:\n",
    "\n",
    "* LLM: underlying model takes a string as input and returns a string\n",
    "* ChatModel: underlying model takes a list of messages as input and returns a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import os, getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message unlike strings is an object of type 'BaseMessage' and is composed of two attributes\n",
    "\n",
    "* content: The content of the message. Usually a string.\n",
    "* role: The entity from which the BaseMessage is coming.\n",
    "\n",
    "Langchain provides several types of messages to distinguish between the roles:\n",
    "\n",
    "* HumanMessage: A BaseMessage coming from a human/user.\n",
    "* AIMessage: A BaseMessage coming from an AI/assistant.\n",
    "* SystemMessage: A BaseMessage coming from the system.\n",
    "* FunctionMessage / ToolMessage: A BaseMessage containing the output of a function or tool call."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to call an LLM or ChatModel is using .invoke()\n",
    "\n",
    "* LLM.invoke: Takes in a string, returns a string.\n",
    "* ChatModel.invoke: Takes in a list of BaseMessage, returns a BaseMessage.\n",
    "\n",
    "Lets see how we can use the 'invoke' method to initiate a response from both types of AI models llm and chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In a /24 subnet, there are 256 total IP addresses. However, the first and last IP addresses are reserved for network address and broadcast address, leaving 254 usable host IP addresses. So, under subnet 192.168.10.0/24, there are 254 usable host IP addresses.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 57, 'total_tokens': 120, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0140cb80-8b67-4992-918d-86e92bae7c36-0', usage_metadata={'input_tokens': 57, 'output_tokens': 63, 'total_tokens': 120})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text_input = \"how many usable host IP addresses are under subnet 192.168.10.0/24?\"\n",
    "system_message = \"\"\"\n",
    "As an expert in the field of computer networking answer all the user related questions. \\\n",
    "try to keep the answers short and concise.\n",
    "\"\"\"\n",
    "message = [HumanMessage(content=text_input), SystemMessage(content=system_message)]\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "#llm.invoke(text_input)\n",
    "chat_model.invoke(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates are used to format user inputs to the LLMs. Instead of simply providing a text/string input like we did above, templates allow us to insert placeholders in to the text.\n",
    "\n",
    "The value for these placeholders can then be provided at runtime when the llm is called. Lets see that with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"SFO_Router\" '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"suggest a {name} for use on a computer networking router based on its location at {location}\")\n",
    "\n",
    "prompt.format(name=\"username\", location=\"san francisco\")\n",
    "#prompt.invoke({\"name\": \"username\", \"location\": \"san francisco\"})\n",
    "\n",
    "llm.invoke(prompt.format(name=\"username\", location=\"san francisco\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates are also available for chat models. For chat models we can put placeholders for inputs from each type of role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"SFNetRouter\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 52, 'total_tokens': 57, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ae9bdef-a677-4e05-a24b-7e45a4791460-0', usage_metadata={'input_tokens': 52, 'output_tokens': 5, 'total_tokens': 57})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = system_message\n",
    "human_template = \"suggest a {name} for use on a computer networking router located in {location}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(name=\"username\", location=\"san francisco\")\n",
    "\n",
    "chat_model.invoke(chat_prompt.format_messages(name=\"username\", location=\"san francisco\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain provides a lot of flexibility when it comes to formatting the prompts. Here's another way of constructing a similar chat promt using differnt types of message objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"SFNetMaster\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 52, 'total_tokens': 57, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a0a69107-0eee-496f-ba83-2da2fb16656c-0', usage_metadata={'input_tokens': 52, 'output_tokens': 5, 'total_tokens': 57})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\"As an expert in the field of computer networking answer all the user related questions. \\\n",
    "                     Try to keep the answers short and concise.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"suggest a {name} for use on a computer networking router located in {location}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_model.invoke(chat_template.format_messages(name=\"username\", location=\"san francisco\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output from the LLMs are plain text but sometimes we desire to get a more structured output from our applications rather than just plain text.\n",
    "\n",
    "This is where the output parsers are very helpful. In Langchain , Outputparsers are python interfaces that must implement the following two methods\n",
    "\n",
    "* parse\n",
    "* get_format_instructions\n",
    "\n",
    "Lets write a very basic Output Parser and then go into a more common way of defining a Langchain Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "class OutputListFormatter(BaseOutputParser):\n",
    "    \"\"\" A class that formats the output from the LLM into a python list\"\"\"\n",
    "\n",
    "    def parse(self, text_input: str) -> List[str]:\n",
    "        return text_input.split(\" \")\n",
    "\n",
    "llm_response = llm.invoke(\"Describe weather forcast for today in San Francisco in two short sentences\")\n",
    "OutputListFormatter().parse(llm_response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example , we defined our own output parser. Langchain however has several pre-defined output parser classes that we can consume , instead of defining our own.\n",
    "\n",
    "One of the popular output parser is \"PydanticOutputParser\" class. Let's look at an example of using this parser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n{\\n  \"query\": \"Describe weather forcast for today at San Francisco\",\\n  \"response\": \"Today in San Francisco, the weather will be mostly cloudy with a high of 64°F and a low of 55°F. There is a 20% chance of rain and winds of up to 10mph. Overall, it will be a cool and cloudy day.\"\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherForcast(BaseModel):\n",
    "    query:str = Field(description=\"question asked to LLM Model\")\n",
    "    response:str = Field(description=\"response to the query from LLM model\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=WeatherForcast)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{format_instructions}\\nDescribe weather forcast for today at {location}\\\n",
    "          in no more than two short sentences\",\n",
    "    input_variables=[\"location\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "llm(prompt.format(location=\"San Francisco\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been passing the prompt.format() method to create an instance of LLM, for. e.g \"llm(prompt.format(location=\"San Francisco\"))\" used in the above cell.\n",
    "\n",
    "Langchain also provides what is known as \"Langchain Expression Language or LCEL\" which uses the \" | \" operator to combine different components together, such as prompts and instance of LLM. LCEL is a way to create arbitrary custom chains.\n",
    "\n",
    "Lets see that with an example. We are going to use the same \"prompt\" that we created in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_and_model_combined = prompt | llm\n",
    "response = prompt_and_model_combined.invoke({\"location\": \"San Francisco\"})\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we combined or chained prompt and llm, we can also combine output parser to form one single chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_chain = prompt | llm | output_parser\n",
    "weather_chain.invoke({\"location\": \"San Francisco\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
