{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building With Langchain\n",
    "\n",
    "The simplest and most common chain contains three things:\n",
    "\n",
    "* LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
    "* Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
    "* Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM/Chat Model\n",
    "\n",
    "There are two types of language models:\n",
    "\n",
    "* LLM: underlying model takes a string as input and returns a string\n",
    "* ChatModel: underlying model takes a list of messages as input and returns a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message unlike strings is an object of type 'BaseMessage' and is composed of two attributes\n",
    "\n",
    "* content: The content of the message. Usually a string.\n",
    "* role: The entity from which the BaseMessage is coming.\n",
    "\n",
    "Langchain provides several types of messages to distinguish between the roles:\n",
    "\n",
    "* HumanMessage: A BaseMessage coming from a human/user.\n",
    "* AIMessage: A BaseMessage coming from an AI/assistant.\n",
    "* SystemMessage: A BaseMessage coming from the system.\n",
    "* FunctionMessage / ToolMessage: A BaseMessage containing the output of a function or tool call."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to call an LLM or ChatModel is using .invoke()\n",
    "\n",
    "* LLM.invoke: Takes in a string, returns a string.\n",
    "* ChatModel.invoke: Takes in a list of BaseMessage, returns a BaseMessage.\n",
    "\n",
    "Lets see how we can use the 'invoke' method to initiate a response from both types of AI models llm and chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='SFGateway', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 52, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d97ff4cd-93b4-4cb9-aa5b-c920841e9557-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text_input = \"suggest a hostname for a computer network router located in san francisco\"\n",
    "system_message = \"\"\"\n",
    "\n",
    "Your job is to provide a short username or hostnames based on where the user or device is located.\n",
    "provide a single output only.\n",
    "\"\"\"\n",
    "message = [HumanMessage(content=text_input), SystemMessage(content=system_message)]\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "#llm.invoke(text_input)\n",
    "chat_model.invoke(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates are used to format user inputs to the LLMs. Instead of simply providing a text/string input like we did above, templates allows us to insert placeholders in to the text.\n",
    "\n",
    "The value for these placeholders can then be provided at runtime when the llm is called. Lets see that with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AI-Projects/langchain-experiments/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"SFRouterTech\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"suggest a {name} for use on a computer networking router located in {location}\")\n",
    "\n",
    "prompt.format(name=\"username\", location=\"san francisco\")\n",
    "#prompt.invoke({\"name\": \"username\", \"location\": \"san francisco\"})\n",
    "\n",
    "llm(prompt.format(name=\"username\", location=\"san francisco\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates are also available for chat models. For chat models we can put placeholders for inputs from each type of role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AI-Projects/langchain-experiments/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = system_message\n",
    "human_template = \"suggest a {name} for use on a computer networking router located in {location}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(name=\"username\", location=\"san francisco\")\n",
    "\n",
    "chat_model_response = chat_model(chat_prompt.format_messages(name=\"username\", location=\"san francisco\"))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain provides a lot of flexibility when it comes to formatting the prompts. Here's another way of constructing a similar chat promt using differnt types of message objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_model(chat_template.format_messages(text=\"i dont like eating tasty things.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output from the LLMs are plain text but sometimes we desire to get a more structured output from our applications rather than just plain text.\n",
    "\n",
    "This is where the output parsers are very helpful. In Langchain , Outputparsers are python interfaces that must implement the following two methods\n",
    "\n",
    "* parse\n",
    "* get_format_instructions\n",
    "\n",
    "Lets write a very basic Output Parser and then go into a more common way of defining a Langchain Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "class OutputListFormatter(BaseOutputParser):\n",
    "    \"\"\" A class that formats the output from the LLM into a python list\"\"\"\n",
    "\n",
    "    def parse(self, text_input: str) -> List[str]:\n",
    "        return text_input.split(\" \")\n",
    "\n",
    "llm_response = llm.invoke(\"Describe weather forcast for today in San Francisco in two short sentences\")\n",
    "OutputListFormatter().parse(llm_response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example , we defined our own output parser. Langchain however has several pre-defined output parser classes that we can consume , instead of defining our own.\n",
    "\n",
    "One of the popular output parser is \"PydanticOutputParser\" class. Let's look at an example of using this parser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherForcast(BaseModel):\n",
    "    query:str = Field(description=\"question asked to LLM Model\")\n",
    "    response:str = Field(description=\"response to the query from LLM model\")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=WeatherForcast)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{format_instructions}\\nDescribe weather forcast for today at {location}\\\n",
    "          in no more than two short sentences\",\n",
    "    input_variables=[\"location\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "llm(prompt.format(location=\"San Francisco\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been passing the prompt.format() method to create an instance of LLM, for. e.g \"llm(prompt.format(location=\"San Francisco\"))\" used in the above cell.\n",
    "\n",
    "Langchain also provides what is known as \"Langchain Expression Language or LCEL\" which uses the \" | \" operator to combine different components together, such as prompts and instance of LLM. LCEL is a way to create arbitrary custom chains.\n",
    "\n",
    "Lets see that with an example. We are going to use the same \"prompt\" that we created in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_and_model_combined = prompt | llm\n",
    "response = prompt_and_model_combined.invoke({\"location\": \"San Francisco\"})\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we combined or chained prompt and llm, we can also combine output parser to form one single chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_chain = prompt | llm | output_parser\n",
    "weather_chain.invoke({\"location\": \"San Francisco\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a27b9a44dddb1eedec426fa966a46221b6963a983add3b1046110dc32ee2722c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
