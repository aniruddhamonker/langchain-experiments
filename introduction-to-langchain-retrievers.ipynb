{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Langchain Retrievers\n",
    "\n",
    "A retriever is an interface that returns documents based on an unstructured query input from a user. \n",
    "\n",
    "It is more general than a vector store and does not need to store documents, only to retrieve them. Retrievers can be used to create retrieval chains that retrieve documents and then pass them on.\n",
    "\n",
    "Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well. For our use case we are going to use a vector store based retriever.\n",
    "\n",
    "Different types of Langchain Retrievers can be found here: [ Langchain_Retrievers ](https://python.langchain.com/docs/modules/data_connection/retrievers)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2262, which is longer than the specified 1000\n",
      "Created a chunk of size 1080, which is longer than the specified 1000\n",
      "Created a chunk of size 1031, which is longer than the specified 1000\n",
      "Created a chunk of size 1047, which is longer than the specified 1000\n",
      "Created a chunk of size 2554, which is longer than the specified 1000\n",
      "Created a chunk of size 3139, which is longer than the specified 1000\n",
      "Created a chunk of size 2846, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "#! pip install faiss-cpu\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "web_document = WebBaseLoader(\"https://www.rfc-editor.org/rfc/rfc6517.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "web_document_chunks = text_splitter.split_documents(web_document)\n",
    "faiss_db = FAISS.from_documents(web_document_chunks, OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell , we are loading a document from the web , splitting it into chunks and then indexing the data using a vectorstore. we have already covered these steps in detail in our previous Jupyter Notebook \"Introduction to Vectorstores\"\n",
    "\n",
    "The only difference here though is we are using another very popular vectorstore named \"FAISS\".\n",
    "\n",
    "With all the necessary steps completed, creating a retriever is fairly easy with just two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_db.as_retriever()\n",
    "query = \"What is the document about?\"\n",
    "\n",
    "#retriever.invoke(query)\n",
    "\n",
    "# from langchain.retrievers import MultiQueryRetriever\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# chat_model = ChatOpenAI(api_key=OPENAI_API_KEY)\n",
    "# multi_query_retriever = MultiQueryRetriever.from_llm(retriever=faiss_db.as_retriever(), llm=chat_model)\n",
    "# docs = multi_query_retriever.invoke(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that a retriever does not get response from a LLM. Its function is primarily to retrieve most relevant documents from a data source based on the user query.\n",
    "\n",
    "In order to create a LLM response to a user query we have to construct a prompt and then pass in the prompt along with the retrived documents from a retriever to a LLM; and we do this by creating a \"Retrieval chain\".\n",
    "\n",
    "The entire flow of this process from ingesting a document , splitting , vectorizing and passing relevant data to LLM is known as \"RAG - Retrieval Augmented Generation\"\n",
    "\n",
    "We have already seen under \"quick_introduction\" notebook that langchain uses LCEL (Langchain Expression Language) to create a chain.\n",
    "\n",
    "chain = llm | prompt | output_parser \n",
    "\n",
    "Now, LCEL is great for constructing simple chains as shown above. For retrieval type of chains which involves adding an extra piece i.e a retriever to the chain, LangChain offers a higher-level constructor method to simplify creating a retrieval based chains. However, all that is being done under the hood is constructing a chain with LCEL.\n",
    "\n",
    "For RAG systems, Langchain offers \"create_retrieval_chain\" constructor method. This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is about multicast VPN proposals and implementations.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"  )\n",
    "\n",
    "#Another way of using promtptemplates\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessagePromptTemplate.from_template(\n",
    "#         \"\"\"Answer the following user question based only on the provided context:\n",
    "# <context>\n",
    "# {context}\n",
    "# </context> \"\"\"),\n",
    "# HumanMessagePromptTemplate.from_template(f\"Question: {input}\")])\n",
    "\n",
    "chat_model_chain = chat_model | prompt\n",
    "document_chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"what document is this about?\"})\n",
    "\n",
    "print(response['answer'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Conversational Retrieval Chain\n",
    "\n",
    "The chain we created above, can answer user questions related to the context of the data source provided. However , the chain does not track any history of prior conversations or questions that are asked.\n",
    "\n",
    "Every question is treated as a new question and is answered without any context or prior knowledge of conversations that happed earlier with the user. This is a problem when you are trying to build an chatbot app. For a chatbot app it is necessary to have history of the previous conversations when answering any new user questions for accurate results.\n",
    "\n",
    "The key is to save the user and the LLM chat history as a variable or entity in our prompt. The new chain will then take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"Answer the following user question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context> \"\"\"),\n",
    "HumanMessagePromptTemplate.from_template(f\"Question: {input}\"),\n",
    "MessagesPlaceholder(variable_name=\"chat_history\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat_model, conversational_prompt)\n",
    "\n",
    "chat_history: List[BaseMessage] = []\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"what is the document about?\",\n",
    "    \"chat_history\":chat_history})\n",
    "\n",
    "chat_history = [AIMessage(content=str(response))]\n",
    "\n",
    "retrieval_chain.invoke({\n",
    "    \"input\": \"do you remember my last question?\",\n",
    "    \"chat_history\":chat_history})\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e98b47d634abda7709beb054e249b01086878e222a92b07cfb67a385a72c7b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
