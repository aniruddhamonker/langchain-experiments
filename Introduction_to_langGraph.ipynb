{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangGraph\n",
    "\n",
    "LangGraph is a library from Langchain for building statful agents and multi-agent workflows.\n",
    "\n",
    "LangGraph unlike other popular agent centric libraries like  \"[Crew](https://www.crewai.com/)\" and \"[Autogen](https://microsoft.github.io/autogen/)\", provides a fine-grained control over the flow and state of your LLM application by utilizing a graph based approach to define the flow of your application.\n",
    "\n",
    "LangGraph is shipped as a complete independent package and does not depend on Langchain for its operation although it does integrate seamlessly with Langchain.\n",
    "\n",
    "\n",
    "## Agents\n",
    "\n",
    "Before we dive into developing an application with LangGraph , we first need to understand what an \"Agent\" is.\n",
    "\n",
    "In our previous notebook \"Langchain Tools\", we have seen how a LLM which supports tool calling functionality can take in a list of tools and based on the user query , decides which of the available tools is most appropriate to answer the query. An agent is a extension to this tool calling functionality. \n",
    "\n",
    "Besides just returning the most appropriate tool to use, an agent takes an action and executes the selected tool with appropriate arguments. It then, processes the LLM response and determines if the response received answers the user query or it needs further evaluation or likely another tool to take the next step towards the final answer. An agent can run in this decision making loop until final answer to user query is acheived.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LangGraph Works\n",
    "As mentioned earlier, LangGraph implements an agent or a multi-agent workflows using graphs. By definition, a Graph is a collection of vertices called \"**Nodes**\" and links called \"**Edges**\" that connect together pairs of nodes .At the heart of every graph lies the \"**State**\" of the graph which represents the current state of the application and is passed along between the Nodes at every step as the graph unfolds. In another words , the Nodes communicate with each other by reading and writing to this shared \"State\". \n",
    "\n",
    "Here's a basic low level flow of a LangGraph Application\n",
    "\n",
    "<img src = \"./images/LangGraph_flow.jpg\" width=\"800\" height=\"400\">\n",
    "\n",
    "Lets start by building a simple LangGraph application that will search the internet and find if there is a valid RFC document that exists based on the user query and then return the URL for that RFC document. We will make use of a predefined tool in Langchain called \"**TavilySearchResults**\" to help LLM find relevant information.\n",
    "\n",
    "(**NOTE:** I will highly recommend going through **\"Langchain Tools\"** notebook first before continuing further in order to understand more about the tools and how to use them if you have not already done so. The remainder of this notebook assumes you are familier with tool calling and how langchain uses \"bind_tools\" function to invoke a tool.)\n",
    "\n",
    "As we build our LangGraph , we will dive deeper into each of the above key constructs of \"State\", \"Nodes\" and \"Edges\"; but first lets do some prep work by installing and importing necesary packages, setting up our API keys , defining the tool which can search the internet and finally initializing our tool calling LLM model.\n",
    "\n",
    "**Note**: \n",
    "* We are going to use Open AI's GPT-4-Tubo model for our use case by installing \"langchain_openai\" package. You can use any other model that you like. Langchain has packages for LLMs from different vendors.\n",
    "* Since we are using an external tool \"TavilySearchResults\" , you will need an external python package \"tavily-python\" and also create an API key by visiting their website [here](https://tavily.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langgraph langsmith langchain_openai langchain_community tavily-python\n",
    "\n",
    "# We are using Langsmith here for visibility and LLM response tracing\n",
    "# but its not required for LangGraph\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\"\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=2)\n",
    "tools = [tavily_tool]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Optional - testing for output from llm\n",
    "# llm_with_tools.invoke(\"what is the weather in san francisco?\")\n",
    "# tavily_tool.invoke(\"what is the weather in Dublin, california?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all things setup and initialized , lets get started !!\n",
    "\n",
    "### State\n",
    "The very first thing we define when builing a langGraph application is the \"State\" class. Think of State as a container that will keep a track and update current state of all the things necessary to make a decision at every step of the graph. A decision could be requesting a LLM response based on the current state, or it could be asking LLM to decide which tool to use next based on the current state of the graph.\n",
    "\n",
    "\n",
    "A State of the graph can be declared as any Python Data Structure but Langchain more commonly declares it as a \"TypedDict\" or a \"Pydantic BaseModel\". You should be familier with \"Pydantic BaseModel\" by now as we have used it a few times already in our previous notebook \"**Langchain Tools**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the State class of type \"TypedDict\" with a key \"messages\" of type list. We are also type hinting \"messages\"  with the type \"Annotated\". The annotation allows us to provide additional reference or information on the variable or object we are defining. In this case we are passing in a built in langgraph \"**add_messages**\" function which defines how the messages object should be updated.\n",
    "\n",
    "Note that \"add_messages\" is a function that we are importing from \"langgraph\" library. Langchain calls these functions \"Reducer Functions\" whose sole purpose is to define how the updates should be applied to the current state. The \"add_messages\" function appends messages to the list rather than overwriting its contents as and when it gets updated. This is important as the State gets passed between different nodes as the graph unfolds,  it is important to keep a reference of all the past messages in the State instead of just the most recent or last message.\n",
    "\n",
    "One thing to note here is that we have only one key (\"messages\") defined in our State. However depending on your application , and the things you need to keep track of during the entire lifecycle of the graph, you may need more keys defined in your State accordingly.\n",
    "\n",
    "we will get back to \"graph_builder\" object later in this notebook when we actually compile and run the graph. For now , just make a note that we are passing our newly defined \"State\" class as a parameter to \"Stategraph\" class that is imported again from \"langgraph\".\n",
    "\n",
    "Okay, the State of th graph is now defined. As you will see, this state will be initialized when the graph is started and then passed along different nodes during the lifecycle of the graph until completion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "Nodes are just simple Python functions that you will define to interact with the LLM. These functions will typically include your \"**llm.invoke**\" methods to get responses from the LLM. At a very basic level they take in current state of the graph as input; interact with LLM based on that current state ; and update the state with the response it receives from LLM.\n",
    "\n",
    "When a node returns and updates the State, the list object \"messages\" that we defined in the \"State\" class will be updated.\n",
    "\n",
    "Although Langchain does not explicitly define it, but looking at multiple examples of LangGraph executions, we can generalize and categorize these nodes under 4 types:\n",
    "\n",
    "* Agent Node\n",
    "* Tool Node\n",
    "* START Node\n",
    "* END Node\n",
    "\n",
    "#### Agent Node\n",
    "Agent Nodes interact with LLM using \"invoke\" method by providing a prompt and an optional list of available tools. If your application uses tools which is a more likely scenario, then Langchain strongly recommends using a LLM model that supports tool calling functionality.\n",
    "\n",
    "The main purpose behind an Agent node is to prompt a LLM based on the current state of the graph and receive necessary information that decides next steps a graph should take. What next steps a graph takes depends on whether the response includes any tool information or not. If LLM decides it needs to call one of the available tools as a next step, it will include the tool name along with all the arguments that are required to call that tool. We will see this information under \"**tool_calls**\" object in the response from LLM.\n",
    "\n",
    "Here's a basic schema for a \"tool_calls\" object\n",
    "\n",
    "``` \n",
    "tool_calls=[\n",
    "        {\n",
    "        'name': 'tavily_search_results_json', \n",
    "        'args': {'query': 'current weather in San Francisco'}, \n",
    "        'id': 'call_ijG8ny0FU3saCvWLasQanZRY', \n",
    "        'type': 'tool_call'\n",
    "    }\n",
    "]\n",
    "```\n",
    "It is important to pay attention here that an agent node does not call a tool, it only provides necessary information to call a tool. We will see this as we progress that it is the \"**Tool Node**\" that uses the \"tool_calls\" object to call and execute the tool.\n",
    "\n",
    "Lets define a Agent Node for our \"RFC Finder\" app that takes in the State as input and invokes LLM for a response based on the information in the list state[\"messages\"]. Keep in mind that the LLM we are using here is already equipped with the list of tools using \"bind_tools\" funtion that we already declared earlier. Once we define our Agent node, we add it to our graph using \"add_node()\" method of \"graph_builder\" object.\n",
    "\n",
    "**Note**: You can check [here](\"https://python.langchain.com/v0.2/docs/integrations/chat/?ref=blog.langchain.dev\") for a list of models that supports tool calling functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "graph_builder.add_node(\"agent_node\", agent_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Node\n",
    "A Tool Node is very similar to an Agent Node in that its just another python function that you define prompting a response from LLM. The difference however is that a tool node actually calls the tool using information from the most recent \"tool_calls\" object under \"State[\"messages\"].\n",
    "\n",
    "At a very basic level,  a Tool Node will include \"**tool_call[\"name\"].invoke(tool_call[\"args\"])**\" method call. If we want , we can define this function ourselves; but Langchain has already done this prepwork for us and  implemented this functionality  by providing a \"ToolNode\" builtin class.\n",
    "\n",
    "Since we have already defined our tool \"**tavily_tool**\" earlier, we are going to provide this tool as an agrument to instantiate \"ToolNode\" class. Finally, just like we added our Agent node to the graph, we will also add our Tool node to the graph using \"add_node\" method under \"graph_builder\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools= tools)\n",
    "\n",
    "graph_builder.add_node(\"tool_node\", tool_node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e98b47d634abda7709beb054e249b01086878e222a92b07cfb67a385a72c7b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
